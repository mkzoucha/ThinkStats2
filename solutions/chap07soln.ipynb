{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples and Exercises from Think Stats, 2nd Edition\n",
    "\n",
    "http://thinkstats2.com\n",
    "\n",
    "Copyright 2016 Allen B. Downey\n",
    "\n",
    "MIT License: https://opensource.org/licenses/MIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'brfss'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-3c2aff517a21>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mbrfss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mthinkstats2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'brfss'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import brfss\n",
    "\n",
    "import thinkstats2\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots\n",
    "\n",
    "I'll start with the data from the BRFSS again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = brfss.ReadBrfss(nrows=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function selects a random subset of a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SampleRows(df, nrows, replace=False):\n",
    "    indices = np.random.choice(df.index, nrows, replace=replace)\n",
    "    sample = df.loc[indices]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll extract the height in cm and the weight in kg of the respondents in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = SampleRows(df, 5000)\n",
    "heights, weights = sample.htm3, sample.wtkg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple scatter plot with `alpha=1`, so each data point is fully saturated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.Scatter(heights, weights, alpha=1)\n",
    "thinkplot.Config(xlabel='Height (cm)',\n",
    "                 ylabel='Weight (kg)',\n",
    "                 axis=[140, 210, 20, 200],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fall in obvious columns because they were rounded off.  We can reduce this visual artifact by adding some random noice to the data.\n",
    "\n",
    "NOTE: The version of `Jitter` in the book uses noise with a uniform distribution.  Here I am using a normal distribution.  The normal distribution does a better job of blurring artifacts, but the uniform distribution might be more true to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jitter(values, jitter=0.5):\n",
    "    n = len(values)\n",
    "    return np.random.normal(0, jitter, n) + values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heights were probably rounded off to the nearest inch, which is 2.8 cm, so I'll add random values from -1.4 to 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = Jitter(heights, 1.4)\n",
    "weights = Jitter(weights, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the jittered data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.Scatter(heights, weights, alpha=1.0)\n",
    "thinkplot.Config(xlabel='Height (cm)',\n",
    "                 ylabel='Weight (kg)',\n",
    "                 axis=[140, 210, 20, 200],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are gone, but now we have a different problem: saturation.  Where there are many overlapping points, the plot is not as dark as it should be, which means that the outliers are darker than they should be, which gives the impression that the data are more scattered than they actually are.\n",
    "\n",
    "This is a surprisingly common problem, even in papers published in peer-reviewed journals.\n",
    "\n",
    "We can usually solve the saturation problem by adjusting `alpha` and the size of the markers, `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.Scatter(heights, weights, alpha=0.1, s=10)\n",
    "thinkplot.Config(xlabel='Height (cm)',\n",
    "                 ylabel='Weight (kg)',\n",
    "                 axis=[140, 210, 20, 200],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better.  This version of the figure shows the location and shape of the distribution most accurately.  There are still some apparent columns and rows where, most likely, people reported their height and weight using rounded values.  If that effect is important, this figure makes it apparent; if it is not important, we could use more aggressive jittering to minimize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to a scatter plot is something like a `HexBin` plot, which breaks the plane into bins, counts the number of respondents in each bin, and colors each bin in proportion to its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.HexBin(heights, weights)\n",
    "thinkplot.Config(xlabel='Height (cm)',\n",
    "                 ylabel='Weight (kg)',\n",
    "                 axis=[140, 210, 20, 200],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the binned plot does a pretty good job of showing the location and shape of the distribution.  It obscures the row and column effects, which may or may not be a good thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  So far we have been working with a subset of only 5000 respondents.  When we include the entire dataset, making an effective scatterplot can be tricky.  As an exercise, experiment with `Scatter` and `HexBin` to make a plot that represents the entire dataset well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# With smaller markers, I needed more aggressive jittering to\n",
    "# blur the measurement artifacts\n",
    "\n",
    "# With this dataset, using all of the rows might be more trouble\n",
    "# than it's worth.  Visualizing a subset of the data might be\n",
    "# more practical and more effective.\n",
    "\n",
    "heights = Jitter(df.htm3, 2.8)\n",
    "weights = Jitter(df.wtkg2, 1.0)\n",
    "\n",
    "thinkplot.Scatter(heights, weights, alpha=0.01, s=2)\n",
    "thinkplot.Config(xlabel='Height (cm)',\n",
    "                 ylabel='Weight (kg)',\n",
    "                 axis=[140, 210, 20, 200],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting percentiles\n",
    "\n",
    "Sometimes a better way to get a sense of the relationship between variables is to divide the dataset into groups using one variable, and then plot percentiles of the other variable.\n",
    "\n",
    "First I'll drop any rows that are missing height or weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df.dropna(subset=['htm3', 'wtkg2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll divide the dataset into groups by height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(135, 210, 5)\n",
    "indices = np.digitize(cleaned.htm3, bins)\n",
    "groups = cleaned.groupby(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the number of respondents in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, group in groups:\n",
    "    print(i, len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the CDF of weight within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heights = [group.htm3.mean() for i, group in groups]\n",
    "cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then extract the 25th, 50th, and 75th percentile from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percent in [75, 50, 25]:\n",
    "    weight_percentiles = [cdf.Percentile(percent) for cdf in cdfs]\n",
    "    label = '%dth' % percent\n",
    "    thinkplot.Plot(mean_heights, weight_percentiles, label=label)\n",
    "    \n",
    "thinkplot.Config(xlabel='Height (cm)',\n",
    "                 ylabel='Weight (kg)',\n",
    "                 axis=[140, 210, 20, 200],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Yet another option is to divide the dataset into groups and then plot the CDF for each group.  As an exercise, divide the dataset into a smaller number of groups and plot the CDF for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "bins = np.arange(140, 210, 10)\n",
    "indices = np.digitize(cleaned.htm3, bins)\n",
    "groups = cleaned.groupby(indices)\n",
    "cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]\n",
    "\n",
    "thinkplot.PrePlot(len(cdfs))\n",
    "thinkplot.Cdfs(cdfs)\n",
    "thinkplot.Config(xlabel='Weight (kg)',\n",
    "                 ylabel='CDF',\n",
    "                 axis=[20, 200, 0, 1],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the covariance of two variables using NumPy's `dot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cov(xs, ys, meanx=None, meany=None):\n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "\n",
    "    if meanx is None:\n",
    "        meanx = np.mean(xs)\n",
    "    if meany is None:\n",
    "        meany = np.mean(ys)\n",
    "\n",
    "    cov = np.dot(xs-meanx, ys-meany) / len(xs)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights, weights = cleaned.htm3, cleaned.wtkg2\n",
    "Cov(heights, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is useful for some calculations, but it doesn't mean much by itself.  The coefficient of correlation is a standardized version of covariance that is easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Corr(xs, ys):\n",
    "    xs = np.asarray(xs)\n",
    "    ys = np.asarray(ys)\n",
    "\n",
    "    meanx, varx = thinkstats2.MeanVar(xs)\n",
    "    meany, vary = thinkstats2.MeanVar(ys)\n",
    "\n",
    "    corr = Cov(xs, ys, meanx, meany) / np.sqrt(varx * vary)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation of height and weight is about 0.51, which is a moderately strong correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr(heights, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy provides a function that computes correlations, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(heights, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a matrix with self-correlations on the diagonal (which are always 1), and cross-correlations on the off-diagonals (which are always symmetric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's correlation is not robust in the presence of outliers, and it tends to underestimate the strength of non-linear relationships.\n",
    "\n",
    "Spearman's correlation is more robust, and it can handle non-linear relationships as long as they are monotonic.  Here's a function that computes Spearman's correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def SpearmanCorr(xs, ys):\n",
    "    xranks = pd.Series(xs).rank()\n",
    "    yranks = pd.Series(ys).rank()\n",
    "    return Corr(xranks, yranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For heights and weights, Spearman's correlation is a little higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpearmanCorr(heights, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas `Series` provides a method that computes correlations, and it offers `spearman` as one of the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpearmanCorr(xs, ys):\n",
    "    xs = pd.Series(xs)\n",
    "    ys = pd.Series(ys)\n",
    "    return xs.corr(ys, method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same as for the one we wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpearmanCorr(heights, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to Spearman's correlation is to transform one or both of the variables in a way that makes the relationship closer to linear, and the compute Pearson's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr(cleaned.htm3, np.log(cleaned.wtkg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from the NSFG, make a scatter plot of birth weight versus mother’s age. Plot percentiles of birth weight versus mother’s age. Compute Pearson’s and Spearman’s correlations. How would you characterize the relationship between these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2002FemPreg.dct'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-a71586c1b519>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfirst\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mlive\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfirsts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mothers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfirst\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMakeFrames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mlive\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msubset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'agepreg'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'totalwgt_lb'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/Shared/OneDrive - Bellevue University/DSC 530 - Data Exploration and Analysis/mkzoucha_dsc530/solutions/first.py\u001B[0m in \u001B[0;36mMakeFrames\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mreturns\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrames\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mall\u001B[0m \u001B[0mlive\u001B[0m \u001B[0mbirths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfirst\u001B[0m \u001B[0mbabies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mothers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \"\"\"\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mpreg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnsfg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReadFemPreg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0mlive\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpreg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutcome\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/Shared/OneDrive - Bellevue University/DSC 530 - Data Exploration and Analysis/mkzoucha_dsc530/solutions/nsfg.py\u001B[0m in \u001B[0;36mReadFemPreg\u001B[0;34m(dct_file, dat_file)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0mreturns\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m     \"\"\"\n\u001B[0;32m---> 50\u001B[0;31m     \u001B[0mdct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthinkstats2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReadStataDct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdct_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReadFixedWidth\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdat_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'gzip'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m     \u001B[0mCleanFemPreg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/Shared/OneDrive - Bellevue University/DSC 530 - Data Exploration and Analysis/mkzoucha_dsc530/thinkstats2/thinkstats2.py\u001B[0m in \u001B[0;36mReadStataDct\u001B[0;34m(dct_file, **options)\u001B[0m\n\u001B[1;32m   2846\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2847\u001B[0m     \u001B[0mvar_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2848\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdct_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2849\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2850\u001B[0m             \u001B[0mmatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch\u001B[0m\u001B[0;34m(\u001B[0m \u001B[0;34mr'_column\\(([^)]*)\\)'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '2002FemPreg.dct'"
     ]
    }
   ],
   "source": [
    "import first\n",
    "\n",
    "live, firsts, others = first.MakeFrames()\n",
    "live = live.dropna(subset=['agepreg', 'totalwgt_lb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "ages = live.agepreg\n",
    "weights = live.totalwgt_lb\n",
    "print('Corr', Corr(ages, weights))\n",
    "print('SpearmanCorr', SpearmanCorr(ages, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'live' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-2b18c8d6826b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m                      xlim=[14, 45], legend=True)\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0mBinnedPercentiles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlive\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'live' is not defined"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def BinnedPercentiles(df):\n",
    "    \"\"\"Bin the data by age and plot percentiles of weight for each bin.\n",
    "\n",
    "    df: DataFrame\n",
    "    \"\"\"\n",
    "    bins = np.arange(10, 48, 3)\n",
    "    indices = np.digitize(df.agepreg, bins)\n",
    "    groups = df.groupby(indices)\n",
    "\n",
    "    ages = [group.agepreg.mean() for i, group in groups][1:-1]\n",
    "    cdfs = [thinkstats2.Cdf(group.totalwgt_lb) for i, group in groups][1:-1]\n",
    "\n",
    "    thinkplot.PrePlot(3)\n",
    "    for percent in [75, 50, 25]:\n",
    "        weights = [cdf.Percentile(percent) for cdf in cdfs]\n",
    "        label = '%dth' % percent\n",
    "        thinkplot.Plot(ages, weights, label=label)\n",
    "\n",
    "    thinkplot.Config(xlabel=\"Mother's age (years)\",\n",
    "                     ylabel='Birth weight (lbs)',\n",
    "                     xlim=[14, 45], legend=True)\n",
    "    \n",
    "BinnedPercentiles(live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-b3f923849b41>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m                      legend=False)\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mScatterPlot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'ages' is not defined"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def ScatterPlot(ages, weights, alpha=1.0, s=20):\n",
    "    \"\"\"Make a scatter plot and save it.\n",
    "\n",
    "    ages: sequence of float\n",
    "    weights: sequence of float\n",
    "    alpha: float\n",
    "    \"\"\"\n",
    "    thinkplot.Scatter(ages, weights)\n",
    "    thinkplot.Config(xlabel='Age (years)',\n",
    "                     ylabel='Birth weight (lbs)',\n",
    "                     xlim=[10, 45],\n",
    "                     ylim=[0, 15],\n",
    "                     legend=False)\n",
    "    \n",
    "ScatterPlot(ages, weights, alpha=0.05, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# My conclusions:\n",
    "\n",
    "# 1) The scatterplot shows a weak relationship between the variables but\n",
    "#    it is hard to see clearly.\n",
    "\n",
    "# 2) The correlations support this.  Pearson's is around 0.07, Spearman's\n",
    "#    is around 0.09.  The difference between them suggests some influence\n",
    "#    of outliers or a non-linear relationsip.\n",
    "\n",
    "# 3) Plotting percentiles of weight versus age suggests that the\n",
    "#    relationship is non-linear.  Birth weight increases more quickly\n",
    "#    in the range of mother's age from 15 to 25.  After that, the effect\n",
    "#    is weaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}